---
title: "NSW Car Crash Severity Analysis"
subtitle: ""
author: 
  - name: "Angela Sajee"
  - name: "Hyungjin Kim"
  - name: "Minji Kim"
  - name: "Rinta Toyoda"
  - name: "Stephen Susanto"
format: 
  revealjs:
    theme: sky
    slide-number: true
    transition: fade
---

```{r, message = FALSE, warning = FALSE}
# list of library
library(readxl)
library(dplyr)
library(ggplot2)
library(rnaturalearth)
library(sf)
library(tidyr)
library(scales)
library(GGally)
library(forcats)
library(patchwork)
library(rlang)
library(caret)
library(knitr) 
library(corrplot)
library(shiny)
library(stringr)
library(devtools)
library(plotly)
library(gridExtra)
library(kableExtra)
library(rpart)
library(caret)
library(dplyr)

# ------------ Check the overall structure of the dataset ---------------#
df_info <- function(df) {
  cat("<class 'data.frame'>\n")
  cat("RangeIndex:", nrow(df), "entries, 0 to", nrow(df)-1, "\n")
  cat("Data columns (total", ncol(df), "columns):\n")
  cat("#   Column                        Non-Null Count  Dtype\n")
  cat("--- ------                        ---------------  -----\n")
  
  for(i in 1:ncol(df)) {
    non_null_count <- sum(!is.na(df[[i]]))
    dtype <- class(df[[i]])[1]
    col_name <- names(df)[i]
    
    cat(sprintf("%-3d %-30s %-15s %s\n", 
                i-1,  # 0-based index
                col_name,
                paste(format(non_null_count, big.mark = ","), "non-null"),
                dtype))
  }
  
  # Data type summary
  cat("\nDtypes:\n")
  dtypes <- sapply(df, function(x) class(x)[1])
  dtype_counts <- table(dtypes)
  for(i in 1:length(dtype_counts)) {
    cat(names(dtype_counts)[i], "(", dtype_counts[i], "), ", sep="")
  }
  cat("\n")
  
  cat("Memory usage:", format(object.size(df), units = "MB"), "\n")
}


# ------------ Check percentage of null values ---------------#
show_null_data <- function(data) {
  if ("sf" %in% class(data)) {
    data <- sf::st_drop_geometry(data)
  }
  
  null_count <- sapply(data, function(x) sum(is.na(x)))
  null_percent <- round(null_count / nrow(data) * 100, 1)
  
  # Build summary dataframe
  null_summary <- data.frame(
    Column = names(data),
    Null_Count = null_count,
    Null_Percent = null_percent,
    row.names = NULL
  )
  
  
  # Sort by Null_Count descending
  null_summary <- null_summary[order(-null_summary$Null_Count), ]
  
  return(null_summary)
}

# ----------- Remove Nulls based on threshold ---------------#
remove_high_nulls <- function(data, threshold = 50) {
  null_summary <- show_null_data(data)
  
  keep_cols <- null_summary$Column[null_summary$Null_Percent <= threshold]
  
  cleaned_data <- data[, keep_cols, drop = FALSE]
  return(cleaned_data)
}


# ----------- Remove Get Mode of Column ---------------#
get_mode <- function(x) {
  ux <- na.omit(unique(x))       
  ux[which.max(tabulate(match(x, ux)))]
}


# -----------Convert column from numerical to categorical and categorical to numerical ------- #
convert_columns <- function(data, num_to_cat = NULL, cat_to_num = NULL) {
  # Copy data to avoid overwriting
  df <- data
  
  if (!is.null(num_to_cat)) {
    for (col in num_to_cat) {
      if (col %in% names(df)) {
        df[[col]] <- as.factor(df[[col]])
      } else {
        warning(paste("Column", col, "not found in dataset"))
      }
    }
  }
  
  if (!is.null(cat_to_num)) {
    for (col in cat_to_num) {
      if (col %in% names(df)) {
        f <- factor(df[[col]], exclude = NULL) 
        df[[col]] <- as.integer(f) - 1
      } else {
        warning(paste("Column", col, "not found in dataset"))
      }
    }
  }
  
  return(as.data.frame(df))  
}

# ----------- remove bad rows based on 5 columns has specific value --------- #
remove_bad_rows <- function(df, threshold = 5) {
  df %>%
    filter(rowSums(
      is.na(.) | . == "Unknown" | . == "None"
    ) <= 5)
}

# ----------- replaced values --------------------#
replace_values <- function(df, column_name, value_if_exists, value_if_na) {
  df[[column_name]] <- ifelse(
    !is.na(df[[column_name]]),   # If value exists
    value_if_exists,             # Replace with this
    value_if_na                  # If NA, replace with this
  )
  return(df)
}

```

```{r, results='hide'}
original_df <- read_excel("nsw_road_crash_data_2019-2023_crash.xlsx")

# Identify numeric columns
numeric_cols <- sapply(original_df, is.numeric)

# Identify categorical columns
categorical_cols <- sapply(original_df, function(x) is.factor(x) | is.character(x))

# Count
num_numeric <- sum(numeric_cols)
num_categorical <- sum(categorical_cols)
```

```{r}
aus <- ne_states(country = "Australia", returnclass = "sf")

nsw <- aus[aus$name == "New South Wales", ]

# Convert crash data to sf points
geo_location <- st_as_sf(original_df, coords = c("Longitude", "Latitude"), crs = 4326, remove = FALSE)

# Keep only points within NSW polygon
geo_location_polygon <- geo_location[st_within(geo_location, nsw, sparse = FALSE), ]

```

```{r, results='hide'}
# ----------- Remove columns that have more than 50% of missing values ------------- #
filtered_null_df <- remove_high_nulls(original_df)

# --------- Remove columns that have high correlation ------------ #
removed_coll_df <- filtered_null_df %>%
  select(-`Reporting year`, -`DCA - code`)


# --------- numerical variable that will be converted to categorical ---------- #
list_column_numerical = c("Year of crash", "Route no.", "RUM - code", "Crash ID")
converted_df_missing_values = convert_columns(data = removed_coll_df, num_to_cat = list_column_numerical)


# ---------- Replace values of Route No ---------- #
# Replace to highway or not_highway based on whether there is a value or not
df_clean_route = replace_values(converted_df_missing_values, "Route no.", value_if_exists = "Highway", value_if_na = "Not Highway")

# ---------- Replace values of Other TU type ----------- #
# Replace to true or false based on whether there is a value or not
df_clean_other = replace_values(df_clean_route, "Other TU type", value_if_exists = "Yes", value_if_na = "No")


# ---------- Delete unreliable rows based on 5 columns with 5 unknown values threshold -----------------#
filtered_null_df <- remove_bad_rows(df_clean_other)


# ------------------------Filtering NSW Map -------------------- #
# Convert crash data to sf points
crash_sf <- st_as_sf(filtered_null_df, coords = c("Longitude", "Latitude"), crs = 4326, remove = FALSE)

# Keep only points within NSW polygon
crash_nsw_polygon <- crash_sf[st_within(crash_sf, nsw, sparse = FALSE), ]

cat("Original rows:", nrow(crash_sf), "\n")
cat("Rows inside NSW polygon:", nrow(crash_nsw_polygon), "\n")

nsw_filtered_df <- as.data.frame(crash_nsw_polygon)


# ----------------- Filtered Out Data ----------------- #
# filtered out year less than 2018
crash_df <- nsw_filtered_df %>%
  filter(`Year of crash` != 2018)

# filtered distance more than 10k
crash_df <- crash_df[crash_df$Distance < 10000, ]


# ----------------- Serious Injury and Fatal become one ---------------#
crash_df$Severity_combined <- ifelse(
  crash_df$`Degree of crash - detailed` %in% c("Fatal", "Serious Injury"),
  "Severe Injury or Fatal",                   
  crash_df$`Degree of crash - detailed`       
)

crash_df <- crash_df %>%
  mutate(
    Time_of_day = case_when(
      `Two-hour intervals` %in% c("04:00 - 05:59", "02:00 - 03:59") ~ "Dawn",
      `Two-hour intervals` %in% c("08:00 - 09:59", "10:00 - 11:59",  "06:00 - 07:59") ~ "Morning",
      `Two-hour intervals` %in% c("12:00 - 13:59", "14:00 - 15:59") ~ "Afternoon",
      `Two-hour intervals` %in% c("18:00 - 19:59", "16:00 - 17:59", "20:00 - 21:59") ~ "Evening",
      `Two-hour intervals` %in% c("22:00 - Midnight", "00:01 - 01:59") ~ "Night",
      `Two-hour intervals` == "Unknown" ~ "Unknown",
      TRUE ~ "Unknown"
    )
  )

converted_df = crash_df
```

## Project Topic

Project: NSW road crash severity analysis

> how environmental and behavioural factors help us predict severity of road crashes in NSW machine learning approach to classify crashes into different levels - no casualty, minor injury, moderate injury, and severe or fatal evaluate how accurate the models are, which predictors are impactful connecting technical results to real-world policy outcomes

# Research Question

## Research Question
> "To what extent can environmental features accurately classify the severity of the car crash, and which variables most significantly influence these patterns?"

## Research Question
-   we are interested in variables that have the greatest influence on those severity patterns.
-   we built a multi-class classification model to categorize crashes into four severity levels
-   identify key factors that influence crash outcomes so the insights can support data-driven safety improvements for community

# Why it Matters?

## Road Safety and Infrastructure

```{r  fig.width=13, fig.height=7, fig.align='center'}

# Australia map
aus <- ne_countries(scale = "large", country = "Australia", returnclass = "sf")

# Plot
ggplot_map <- ggplot() +
  geom_sf(data = aus, fill = "lightgray", color = "white") +
  geom_point(
    data = converted_df, 
    aes(x = Longitude, y = Latitude, color = Severity_combined), 
    size = 3,        # increase marker size
    alpha = 0.8
  ) +
  coord_sf(xlim = c(141, 154), ylim = c(-38, -28), expand = FALSE) +  # NSW bounds
  theme_minimal(base_size = 12) +  # increase base font size
  labs(
    title = "Crash Locations in NSW",
    x = "Longitude", 
    y = "Latitude", 
    color = "Crash Severity"
  ) +
  theme(
    plot.title = element_text(size = 14, face = "bold", hjust = 0.5),
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    legend.title = element_text(size = 11, face = "bold"),
    legend.text = element_text(size = 9)
  )

ggplot_map

```

## Sustainable mobility

```{r}
p1 <- ggplot(converted_df, aes(x = `Severity_combined`)) +
  geom_bar(aes(fill = `Severity_combined`)) +
  scale_fill_manual(values = c(
    "Minor/Other Injury" = "grey60",
    "Moderate Injury" = "grey60",
    "Non-casualty (towaway)" = "grey60",
    "Severe Injury or Fatal" = "grey60"
  )) +
  geom_text(stat='count', aes(label=..count..), vjust=-0.5, size=3) +  # Add counts
  theme_minimal(base_size = 8) +
  labs(title = "Original Severity Distribution", x = "Severity", y = "Count") +
  theme(axis.text.x = element_text(angle=45, hjust=1),
        legend.position="none")

p1
```

## Sustainable Mobility

```{r}
trend <- converted_df %>%
  group_by(`Year of crash`, Severity_combined) %>%
  summarise(count = n(), .groups = 'drop')

ggplot(trend, aes(x = `Year of crash`, y = count, color = Severity_combined, group = Severity_combined)) +
  geom_line(size = 1.2) +
  geom_point(size = 2) +
  theme_minimal(base_size = 12) +
  labs(
    title = "Crash Trend by Severity Over Years",
    x = "Year",
    y = "Number of Crashes",
    color = "Severity"
  )
#TODO: put variance in the line plot

```

## Data-driven decision-making

```{r}
ggplot(converted_df, aes(x = Weather, fill = Severity_combined)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("grey70", "gold", "orange", "red")) +
  theme_minimal(base_size = 11) +
  labs(title = "Crash Severity by Weather Condition",
       x = "Weather Condition", y = "Proportion of Crashes", fill = "Severity") +
  theme(axis.text.x = element_text(angle=45, hjust=1))
# switch the weather and crash severity
```

## Safer infrastructure

```{r}
ggplot(converted_df, aes(x = `Natural lighting`, fill = Severity_combined)) +
  geom_bar(position = "fill") +
  scale_fill_manual(values = c("grey70", "gold", "orange", "red")) +
  theme_minimal(base_size = 11) +
  labs(title = "Crash Severity by Lighting Condition",
       x = "Lighting Condition", y = "Proportion of Crashes", fill = "Severity") +
  theme(axis.text.x = element_text(angle=45, hjust=1))
```

## How to process the data?

-   Removing columns/rows that might cause problems
-   Remove Data Point That not in the average area
-   Handling inconsistent datatype
-   Handle Class Imbalance

```{r}
# Before balancing
ggplot(converted_df, aes(x = `Degree of crash - detailed`)) +
  geom_bar(fill = "orange") +
  labs(title = "Class Distribution Before Balancing")

# After balancing (e.g., using ROSE or SMOTE)
ggplot(converted_df, aes(x = Severity_combined)) +
  geom_bar(fill = "steelblue") +
  labs(title = "Class Distribution After Balancing")
```

## What Does Your Data Look Like?

## What model(s) are you using? Why?

## Key Results and Visuals

## Final Insighs: What have you learned & link back to the research problem
